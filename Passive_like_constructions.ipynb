{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Passive_like_constructions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install SNgramExtractor\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "from SNgramExtractor import SNgramExtractor\n",
        "from nltk.corpus import wordnet as wn\n",
        "import spacy\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "3bHH_TmhsgUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation & Processing Corpora"
      ],
      "metadata": {
        "id": "x1IVe9qVC1kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1RpnEIgCw2G",
        "outputId": "c3ef3b75-987d-40a7-9a99-f3b2d98be01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NB!** Для повторения кода все необходимые материалы, а именно первоначальные корпуса, должны находится на гугл-диске (если не говорится иное / размер корпуса позволяет загрузить его на диск), который будут подключать. Все ссылки на использованные материалы представлены в соотвестсующем подразделе."
      ],
      "metadata": {
        "id": "qDctmCycw3j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leipzig Corpora Collection (News discourse)"
      ],
      "metadata": {
        "id": "4ZMDl9jeEcyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: Leipzig Corpopa Collection - https://wortschatz.uni-leipzig.de/en/download"
      ],
      "metadata": {
        "id": "IUJsMw3uEOSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_corpus_2020 = open(r'/content/drive/MyDrive/Colab Notebooks/eng_news_2020_1M-sentences.txt', encoding='utf-8', errors='ignore').read().split('\\t')\n",
        "news_corpus_2019 = open(r'/content/drive/MyDrive/Colab Notebooks/eng_news_2019_1M-sentences.txt', encoding='utf-8', errors='ignore').read().split('\\t')\n",
        "news_corpus_2018 = open(r'/content/drive/MyDrive/Colab Notebooks/eng_news_2018_1M-sentences.txt', encoding='utf-8', errors='ignore').read().split('\\t')\n",
        "news_corpus_2017 = open(r'/content/drive/MyDrive/Colab Notebooks/eng_news_2017_1M-sentences.txt', encoding='utf-8', errors='ignore').read().split('\\t')\n",
        "news_corpus_2016 = open(r'/content/drive/MyDrive/Colab Notebooks/eng_news_2016_1M-sentences.txt', encoding='utf-8', errors='ignore').read().split('\\t')\n",
        "news_corpus_2015 = open(r'/content/drive/MyDrive/Colab Notebooks/eng_news_2015_1M-sentences.txt', encoding='utf-8', errors='ignore').read().split('\\t')"
      ],
      "metadata": {
        "id": "OpDbhsllC_mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_news(corpus):\n",
        "  clean_corpus = []\n",
        "  for sentence in tqdm(corpus[2:]): #первые два предложения в корпусе идут не информативными, поэтому начинаем с 3\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'[\\n]\\d+','', sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,;…0-9]+\", \" \", sentence)\n",
        "    clean_corpus.append(sentence)\n",
        "  return clean_corpus"
      ],
      "metadata": {
        "id": "joTSeSbkE6QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_news_corpora = [news_corpus_2015, news_corpus_2016, news_corpus_2017, news_corpus_2018, news_corpus_2019, news_corpus_2020]\n",
        "news_data = pd.DataFrame(columns=['sentences'])\n",
        "for corpus in list_of_news_corpora:\n",
        "  corpus = preprocessing_news(corpus)\n",
        "  data = pd.DataFrame(columns=['sentences'], data=corpus)\n",
        "  news_data = news_data.append(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJEbHVYPHzie",
        "outputId": "cab28623-7aa9-4ea4-c07e-5c4ad1a21c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 965709/965709 [00:14<00:00, 68226.64it/s]\n",
            "100%|██████████| 999999/999999 [00:10<00:00, 94980.15it/s]\n",
            "100%|██████████| 999999/999999 [00:10<00:00, 93975.53it/s]\n",
            "100%|██████████| 999999/999999 [00:10<00:00, 94226.33it/s]\n",
            "100%|██████████| 999999/999999 [00:10<00:00, 93923.58it/s]\n",
            "100%|██████████| 999999/999999 [00:13<00:00, 73996.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_data.to_csv('news_corpus.csv') #сохраняем все обработанные корпуса для удобства применения на них после алгоритма поиска неморфологического пассива"
      ],
      "metadata": {
        "id": "X1UMpB4qdg9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ACL Anthology Papers (Scientific discourse)"
      ],
      "metadata": {
        "id": "ibclZcF99ggW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Научный дискурс представлен двумя корпуса в связи с тем, что было трудно найти корпус, соизмеримый по размерам с политическим и научным корпусами. Даже использование двух корпусов не помогло, и научный корпус является самым маленьким из тех, что используются в настоящей исследовательской работе."
      ],
      "metadata": {
        "id": "h5CSc_9HxorB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://www.kaggle.com/datasets/takahirokubo0/acl-anthology-papers?select=emnlp-2016.csv"
      ],
      "metadata": {
        "id": "00MmALqq_7pU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scientific_corpus_2016 = pd.read_csv('drive/MyDrive/Colab Notebooks/emnlp-2016.csv')\n",
        "scientific_corpus_2017 = pd.read_csv('drive/MyDrive/Colab Notebooks/emnlp-2017.csv')\n",
        "scientific_corpus_2018 = pd.read_csv('drive/MyDrive/Colab Notebooks/emnlp-2018.csv')"
      ],
      "metadata": {
        "id": "JoacNJt19eID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_scientific_ACL(corpus):\n",
        "  clean_corpus = []\n",
        "  for text in tqdm(corpus['abstract']):\n",
        "    if pd.isnull(text) != True:\n",
        "      text = text.split('.')\n",
        "      for sentence in text:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = re.sub(r\"[^a-zA-Z?.!,;…0-9]+\", \" \", sentence)\n",
        "        clean_corpus.append(sentence)\n",
        "  return clean_corpus"
      ],
      "metadata": {
        "id": "jrzdG-N8A4yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_scientific_corpora = [scientific_corpus_2016, scientific_corpus_2017, scientific_corpus_2018]\n",
        "scientific_data = pd.DataFrame(columns=['sentences'])\n",
        "for corpus in list_of_scientific_corpora:\n",
        "  corpus = preprocessing_scientific_ACL(corpus)\n",
        "  data = pd.DataFrame(columns=['sentences'], data=corpus)\n",
        "  scientific_data = scientific_data.append(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN4GTmZkiJ0g",
        "outputId": "887bc6db-7b42-42d1-aca6-4d285bc3dae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [00:00<00:00, 15354.20it/s]\n",
            "100%|██████████| 324/324 [00:00<00:00, 15600.98it/s]\n",
            "100%|██████████| 550/550 [00:00<00:00, 14525.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PubMed (Scientific corpus)"
      ],
      "metadata": {
        "id": "ze5FLiMTRuvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://github.com/Franck-Dernoncourt/pubmed-rct"
      ],
      "metadata": {
        "id": "DvB-3SnSSTor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pub_med = pd.read_csv('drive/MyDrive/Colab Notebooks/pub_med.csv')"
      ],
      "metadata": {
        "id": "l34AeHmhPiIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processing_scientific_PubMed(corpus):\n",
        "  clean_corpus = []\n",
        "  for sentence in tqdm(corpus):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,;…0-9]+\", \" \", sentence)\n",
        "    clean_corpus.append(sentence)\n",
        "  return clean_corpus"
      ],
      "metadata": {
        "id": "a94Ph2fHQxSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = processing_scientific_PubMed(pub_med['text'])\n",
        "data = pd.DataFrame(columns=['sentences'], data=corpus)\n",
        "scientific_data = scientific_data.append(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xk77B4dis3G",
        "outputId": "a1710f32-c602-4098-f8ba-e3e03074270e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 180040/180040 [00:01<00:00, 97697.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scientific_data.to_csv('scientific_corpus.csv')"
      ],
      "metadata": {
        "id": "HOkOVFFgkJkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Books (Fiction corpus)"
      ],
      "metadata": {
        "id": "CkmGuyslSLnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://github.com/soskek/bookcorpus"
      ],
      "metadata": {
        "id": "b39JJVSSSsmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часть кода ниже была запущена в Jupyter Notebook для работы с локальными файлами компьютера. Выгрузка файлов на гугл диск была бы не возможной из-за размера корпуса."
      ],
      "metadata": {
        "id": "JaBaGOy2yi_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "P94LD3wXyiM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_book_corpus(link_to_the_corpus):\n",
        "    dict_for_data = {'text': []}\n",
        "    data = pd.DataFrame(dict_for_data)\n",
        "    \n",
        "    for filename in os.listdir(link_to_the_corpus):\n",
        "        with open(os.path.join(link_to_the_corpus, filename), 'r') as f:\n",
        "            text = f.read()\n",
        "            new_row = {'text': text}\n",
        "        data = data.append(new_row, ignore_index=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "book_corpus = create_book_corpus('/Users/elena.kislyanicina/Downloads/books1/BooksCorpus/')"
      ],
      "metadata": {
        "id": "5543ApIJyrbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_fiction(corpus):\n",
        "    clean_corpus = []\n",
        "    clean_text = []\n",
        "    for text in tqdm(corpus):\n",
        "        text = sent_tokenize(text)\n",
        "        for sentence in text:\n",
        "            sentence = sentence.lower()\n",
        "            sentence = re.sub(r\"[^a-zA-Z?.!,;…0-9]+\", \" \", sentence)\n",
        "            clean_text.append(sentence)\n",
        "    clean_corpus.append(clean_text)\n",
        "    return clean_corpus"
      ],
      "metadata": {
        "id": "3koC3O3xdJf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_book_corpus = preprocessing_fiction(books_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvGRl857eEsL",
        "outputId": "8a396199-2c2b-4de7-d73b-5e40881f27df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 128/128 [00:25<00:00,  4.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fiction_data = pd.DataFrame(columns=['sentences'])\n",
        "\n",
        "for text in tqdm(clean_book_corpus):\n",
        "  data = pd.DataFrame(columns=['sentences'], data=text)\n",
        "  fiction_data = fiction_data.append(data)"
      ],
      "metadata": {
        "id": "xvEaV-vdezPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a84b26-1ed8-4c85-a973-d28b827a40a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 128/128 [02:20<00:00,  1.10s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fiction_data.to_csv('fiction_corpus.csv')"
      ],
      "metadata": {
        "id": "l2X-9zGyrv-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parliamentary corpora (Political corpus)"
      ],
      "metadata": {
        "id": "sQ9dAeouggWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://www.clarin.eu/resource-families/parliamentary-corpora"
      ],
      "metadata": {
        "id": "Wg06leUshyYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Код по сбору политического корпуса также был запущен в Jupyter Notebook, т.к. был необходим доступ к локальной файловой системе. Выгрузка файлов на гугл диск была бы не возможной из-за размера корпуса."
      ],
      "metadata": {
        "id": "LV91ynixy_7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu\n",
        "from conllu import parse_incr"
      ],
      "metadata": {
        "id": "kxy2FMfzh4vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_political_corpus(link_to_the_corpus):\n",
        "    dict_for_data = {'text': []}\n",
        "    data = pd.DataFrame(dict_for_data)\n",
        "    \n",
        "    for filename in tqdm(os.listdir(link_to_the_corpus)):\n",
        "        if filename.endswith(\".conllu\"):\n",
        "            with open(os.path.join(link_to_the_corpus, filename), 'r', encoding=\"utf-8\") as f:\n",
        "                political_corpus = []\n",
        "                for tokenlist in parse_incr(f):\n",
        "                    list_for_sentence = []\n",
        "                    string = ''\n",
        "                    for word in tokenlist:\n",
        "                        string += word['form'] + ' '\n",
        "                    political_corpus.append(string)\n",
        "                new_row = {'text': political_corpus}\n",
        "            data = data.append(new_row, ignore_index=True)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "ioIUFyMNWCCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "political_corpus_2015 = create_political_corpus('/Users/elena.kislyanicina/Downloads/ParlaMint-GB.ana/ParlaMint-GB.conllu/2015/')\n",
        "political_corpus_2016 = create_political_corpus('/Users/elena.kislyanicina/Downloads/ParlaMint-GB.ana/ParlaMint-GB.conllu/2016/')\n",
        "political_corpus_2017 = create_political_corpus('/Users/elena.kislyanicina/Downloads/ParlaMint-GB.ana/ParlaMint-GB.conllu/2017/')\n",
        "political_corpus_2018 = create_political_corpus('/Users/elena.kislyanicina/Downloads/ParlaMint-GB.ana/ParlaMint-GB.conllu/2018/')\n",
        "political_corpus_2019 = create_political_corpus('/Users/elena.kislyanicina/Downloads/ParlaMint-GB.ana/ParlaMint-GB.conllu/2019/')\n",
        "political_corpus_2020 = create_political_corpus('/Users/elena.kislyanicina/Downloads/ParlaMint-GB.ana/ParlaMint-GB.conllu/2020/')\n",
        "political_corpus_2021 = create_political_corpus('/Users/elena.kislyanicina/Downloads/ParlaMint-GB.ana/ParlaMint-GB.conllu/2021/')"
      ],
      "metadata": {
        "id": "e_zG_tXOznOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_political(corpus):\n",
        "    clean_corpus = []\n",
        "    for text in tqdm(corpus['text']):\n",
        "        for sentence in text:\n",
        "            sentence = sentence.lower()\n",
        "            sentence = re.sub(r\"[^a-zA-Z?.!,;…0-9]+\", \" \", sentence)\n",
        "            clean_corpus.append(sentence)\n",
        "    return clean_corpus"
      ],
      "metadata": {
        "id": "EI_e8s8anLUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_political_corpora = [political_corpus_2015, political_corpus_2016, political_corpus_2017, political_corpus_2018, political_corpus_2019, political_corpus_2020, political_corpus_2021]\n",
        "\n",
        "political_data = pd.DataFrame(columns=['sentences'])\n",
        "\n",
        "for corpus in list_of_political_corpora:\n",
        "    corpus = preprocessing_political(corpus)\n",
        "    data = pd.DataFrame(columns=['sentences'], data=corpus)\n",
        "    political_data = political_data.append(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L92u_9lrnawb",
        "outputId": "f2c71510-cde1-471a-ac0c-698dbf8120cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2872/2872 [00:00<00:00, 81207.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "political_data.to_csv('political_corpus.csv')"
      ],
      "metadata": {
        "id": "cWjxOLD6zzba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finite constructions"
      ],
      "metadata": {
        "id": "bYdCoiDoDAaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finite_constructions(corpus):\n",
        "    dict_for_data = {'INDEX_OF_SENTENCE': [], 'SENTENCE': [], 'CONSTRUCTION': [], 'TYPE_OF_CONSTRUCTION': []}\n",
        "    data = pd.DataFrame(dict_for_data)\n",
        "  \n",
        "    for index, sentence in enumerate(tqdm(corpus)):\n",
        "        SNgram_pos = SNgramExtractor(str(sentence),meta_tag='pos',trigram_flag='yes',nlp_model=None)\n",
        "        output_pos = SNgram_pos.get_SNgram()\n",
        "        SNgram_dep = SNgramExtractor(str(sentence),meta_tag='dep',trigram_flag='yes',nlp_model=None)\n",
        "        output_dep = SNgram_dep.get_SNgram()\n",
        "    \n",
        "        for sn_gram_pos, sn_gram_dep in zip(output_pos['SNBigram'].split(), output_dep['SNBigram'].split()):\n",
        "            \n",
        "            if 'NOUN' in sn_gram_pos and 'VERB' in sn_gram_pos and 'nsubj' in sn_gram_dep and 'nsubjpass' not in sn_gram_dep:\n",
        "                noun = sn_gram_pos.split('_')[2]\n",
        "                try:\n",
        "                    wn.synset('{}.n.01'.format(noun))\n",
        "                except: \n",
        "                    continue\n",
        "                else:\n",
        "                    word = wn.synset('{}.n.01'.format(noun))\n",
        "                    concrete = word.wup_similarity(wn.synset('concrete.n.01'))\n",
        "                    abstract = word.wup_similarity(wn.synset('abstract.n.01'))\n",
        "                    if concrete > abstract:\n",
        "                        boolean_answer1 = wn.synset('person.n.01') in word.lowest_common_hypernyms(wn.synset('person.n.01'))\n",
        "                        boolean_answer2 = wn.synset('people.n.01') in word.lowest_common_hypernyms(wn.synset('people.n.01'))\n",
        "                        boolean_answer5 = wn.synset('animal.n.01') in word.lowest_common_hypernyms(wn.synset('animal.n.01'))\n",
        "                        if boolean_answer1 == False and boolean_answer2 == False and boolean_answer5 == False:\n",
        "                            new_row = {'INDEX_OF_SENTENCE': index, 'SENTENCE': sentence, 'CONSTRUCTION': sn_gram_pos, 'TYPE_OF_CONSTRUCTION': 'finite constructions'}\n",
        "                            data = data.append(new_row, ignore_index=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "8o1Nwm-VLKjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retroactive infinitive"
      ],
      "metadata": {
        "id": "-nq4fDfl8ZJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retroactive_infinitive(corpus):\n",
        "    dict_for_data = {'INDEX_OF_SENTENCE': [], 'SENTENCE': [], 'CONSTRUCTION': [], 'TYPE_OF_CONSTRUCTION': []}\n",
        "    data = pd.DataFrame(dict_for_data)\n",
        "  \n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    for index, sentence in enumerate(tqdm(corpus)):\n",
        "        SNgram_pos = SNgramExtractor(str(sentence),meta_tag='pos',trigram_flag='yes',nlp_model=None)\n",
        "        output_pos = SNgram_pos.get_SNgram()\n",
        "        SNgram_dep = SNgramExtractor(str(sentence),meta_tag='dep',trigram_flag='yes',nlp_model=None)\n",
        "        output_dep = SNgram_dep.get_SNgram()\n",
        "\n",
        "        infinitive_verb = '*'    \n",
        "\n",
        "        for sn_gram in output_pos['SNBigram'].split():\n",
        "            if '_VERB_to_PART' in sn_gram:\n",
        "                infinitive_verb = sn_gram.split('_')[0]\n",
        "                doc = nlp(infinitive_verb)\n",
        "                infinitive_verb = doc[0].lemma_\n",
        "\n",
        "                for sn_gram_pos, sn_gram_dep in zip(output_pos['SNBigram'].split(), output_dep['SNBigram'].split()):\n",
        "        \n",
        "                    checker1 = re.findall(r'(?:is|are|am|was|were|will)_ROOT_{}_xcomp'.format(infinitive_verb), sn_gram_dep)\n",
        "                    checker2 = re.findall(r'(?:is|are|am|was|were|will)_conj_{}_xcomp'.format(infinitive_verb), sn_gram_dep)\n",
        "\n",
        "                    if infinitive_verb in sn_gram_pos and 'NOUN' in sn_gram_pos.split('_')[1] and 'VERB' in sn_gram_pos and 'relcl' in sn_gram_dep and 'dobj' not in sn_gram_dep and 'order' not in sn_gram_pos:\n",
        "                        new_row = {'INDEX_OF_SENTENCE': index, 'SENTENCE': sentence, 'CONSTRUCTION': sn_gram_pos, 'TYPE_OF_CONSTRUCTION': 'retroactive infinitive'}\n",
        "                        data = data.append(new_row, ignore_index=True)\n",
        "                    elif checker1 != [] or checker2 != []:\n",
        "                        \n",
        "                        checker3 = re.findall(r'(?:is|are|am|was|were|will) [a-z]+ to {}'.format(infinitive_verb), sentence)\n",
        "                        checker4 = re.findall(r'(?:is|are|am|was|were|will) [a-z]+ [a-z]+ to {}'.format(infinitive_verb), sentence)\n",
        "                        \n",
        "                        if checker3 != [] or checker4 != []:\n",
        "                            new_row = {'INDEX_OF_SENTENCE': index, 'SENTENCE': sentence, 'CONSTRUCTION': sn_gram_pos, 'TYPE_OF_CONSTRUCTION': 'retroactive infinitive'}\n",
        "                            data = data.append(new_row, ignore_index=True)\n",
        "                    elif infinitive_verb in sn_gram_pos and 'ADJ' in sn_gram_pos.split('_')[1] and 'VERB' in sn_gram_pos and 'xcomp' in sn_gram_dep and 'able' not in sn_gram_pos:\n",
        "                        new_row = {'INDEX_OF_SENTENCE': index, 'SENTENCE': sentence, 'CONSTRUCTION': sn_gram_pos, 'TYPE_OF_CONSTRUCTION': 'retroactive infinitive'}\n",
        "                        data = data.append(new_row, ignore_index=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "RL_IVGnD8gyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gerundial Constructions"
      ],
      "metadata": {
        "id": "fmCGeRh00kGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gerundial_construction(corpus):\n",
        "  dict_for_data = {'INDEX_OF_SENTENCE': [], 'SENTENCE': [], 'CONSTRUCTION': [], 'TYPE_OF_CONSTRUCTION': []}\n",
        "  data = pd.DataFrame(dict_for_data)\n",
        "  \n",
        "  for index, sentence in enumerate(tqdm(corpus)):\n",
        "    SNgram_pos = SNgramExtractor(sentence,meta_tag='pos',trigram_flag='yes',nlp_model=None)\n",
        "    output_pos = SNgram_pos.get_SNgram()\n",
        "\n",
        "    for sn_gram in output_pos['SNBigram'].split():\n",
        "      if 'worth_ADJ' in sn_gram and 'VERB' in sn_gram:\n",
        "        new_row = {'INDEX_OF_SENTENCE': index, 'SENTENCE': sentence, 'CONSTRUCTION': sn_gram, 'TYPE_OF_CONSTRUCTION': 'gerundial construction'}\n",
        "        data = data.append(new_row, ignore_index=True)\n",
        "\n",
        "      checker = re.findall(r'(?:bear|deserve|need|require|want)[a-z]*_VERB_[a-z]+ing_VERB', sn_gram)\n",
        "      if checker != []:\n",
        "        new_row = {'INDEX_OF_SENTENCE': index, 'SENTENCE': sentence, 'CONSTRUCTION': sn_gram, 'TYPE_OF_CONSTRUCTION': 'gerundial construction'}\n",
        "        data = data.append(new_row, ignore_index=True)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "IRotEimx0jTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lexical passive"
      ],
      "metadata": {
        "id": "8jJQFpcq_XEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lexical_passive(corpus):\n",
        "    dict_for_data = {'INDEX_OF_SENTENCE': [], 'SENTENCE': [], 'CONSTRUCTION': [], 'TYPE_OF_CONSTRUCTION': []}\n",
        "    data = pd.DataFrame(dict_for_data)\n",
        "  \n",
        "    for index, sentence in enumerate(tqdm(corpus)):\n",
        "        SNgram_pos = SNgramExtractor(str(sentence),meta_tag='pos',trigram_flag='yes',nlp_model=None)\n",
        "        output_pos = SNgram_pos.get_SNgram()\n",
        "        SNgram_dep = SNgramExtractor(str(sentence),meta_tag='dep',trigram_flag='yes',nlp_model=None)\n",
        "        output_dep = SNgram_dep.get_SNgram()\n",
        "\n",
        "        for sn_gram_pos, sn_gram_dep in zip(output_pos['SNBigram'].split(), output_dep['SNBigram'].split()):\n",
        "            lexical_passive_verb = re.findall(r'(?:receiv|suffer|underg|underw)[a-z]*_VERB_[a-z]+_(?:NOUN|PRON|PROPN|NUM)', sn_gram_pos)\n",
        "\n",
        "            if lexical_passive_verb != [] and 'nsubj' in sn_gram_dep:\n",
        "                new_row = {'INDEX_OF_SENTENCE': index, 'SENTENCE': sentence, 'CONSTRUCTION': sn_gram_pos, 'TYPE_OF_CONSTRUCTION': 'lexical passive'}\n",
        "                data = data.append(new_row, ignore_index=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "46zNvQWj_aCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final corpus of passive-like constructions"
      ],
      "metadata": {
        "id": "XwFE0EF61Y4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данная часть запускалась в Jupyter Notebook, т.к. требовалось значительное время для обработки каждого первоначального корпуса."
      ],
      "metadata": {
        "id": "7_48SBnN1fb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "cHWgaHbc16H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_corpus = pd.read_csv('/Users/elena.kislyanicina/Downloads/news_corpus.csv')\n",
        "scientific_corpus = pd.read_csv('/Users/elena.kislyanicina/Downloads/scientific_corpus.csv')\n",
        "fiction_corpus = pd.read_csv('/Users/elena.kislyanicina/Downloads/fiction_corpus.csv')\n",
        "political_corpus = pd.read_csv('/Users/elena.kislyanicina/Downloads/political_corpus.csv')"
      ],
      "metadata": {
        "id": "pFN7JWHh2V7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_passive_like_constructions(corpus):\n",
        "    \n",
        "    f_constructions = finite_constructions(corpus)\n",
        "    r_i_constructions = retroactive_infinitive(corpus)\n",
        "    g_construction = gerundial_construction(corpus)\n",
        "    l_p_constructions = lexical_passive(corpus)\n",
        "    \n",
        "    frames = [f_constructions, r_i_constructions, g_construction, l_p_constructions]\n",
        "    \n",
        "    data = pd.concat(frames)\n",
        "    \n",
        "    return data"
      ],
      "metadata": {
        "id": "8fAnt62D1YVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scientific_discourse = find_passive_like_constructions(scientific_corpus['sentences'])\n",
        "\n",
        "news_corpus_shuffle = shuffle(news_corpus)\n",
        "news_discourse = find_passive_like_constructions(news_corpus_shuffle['sentences'][:200001])\n",
        "\n",
        "political_corpus_shuffle = shuffle(political_corpus)\n",
        "political_discourse = find_passive_like_constructions(political_corpus_shuffle['sentences'][:200001])\n",
        "\n",
        "fiction_corpus_shuffle = shuffle(fiction_corpus)\n",
        "fiction_discourse = find_passive_like_constructions(fiction_corpus_shuffle['sentences'][:200001])"
      ],
      "metadata": {
        "id": "JKCCARk510RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scientific_discourse.to_csv('scientific_discourse.csv')\n",
        "news_discourse.to_csv('news_discourse.csv')\n",
        "political_discourse.to_csv('political_discourse.csv')\n",
        "fiction_discourse.to_csv('fiction_discourse.csv')"
      ],
      "metadata": {
        "id": "AtA7x1XQ3lNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional processing"
      ],
      "metadata": {
        "id": "1n2sKRKp2w2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дополнительные фильтры, которые были использованы, чтобы снизить процент шума в подкорпусах"
      ],
      "metadata": {
        "id": "ZB0Ii4YN2400"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_d = pd.read_csv('/Users/elena.kislyanicina/Downloads/news_discourse.csv')\n",
        "scientific_d = pd.read_csv('/Users/elena.kislyanicina/Downloads/scientific_discourse.csv')\n",
        "fiction_d = pd.read_csv('/Users/elena.kislyanicina/Downloads/fiction_discourse.csv')\n",
        "political_d = pd.read_csv('/Users/elena.kislyanicina/Downloads/political_discourse.csv')"
      ],
      "metadata": {
        "id": "_6rHr7ET3dpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_1(corpus): #был нужен, т.к. из-за несовсем корректной очистки корпуса было много конструкций, в которых части отрицания считались за существительное\n",
        "    check_list = []\n",
        "    for construction in corpus['CONSTRUCTION']:\n",
        "        if construction.split('_')[2] == 'i' or len(construction.split('_')[2]) > 1: \n",
        "            check_list.append(True)\n",
        "        else:\n",
        "            check_list.append(False)\n",
        "    corpus['check'] = check_list\n",
        "    corpus = corpus.drop(np.where(corpus['check'] == False)[0])\n",
        "    corpus.drop(columns = ['check'],axis = 1, inplace=True)\n",
        "    \n",
        "    return corpus"
      ],
      "metadata": {
        "id": "pc7DIdrY3AoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_d = filter_1(news_d)\n",
        "scientific_d = filter_1(scientific_d)\n",
        "political_d = filter_1(political_d)\n",
        "fiction_d = filter_1(fiction_d)"
      ],
      "metadata": {
        "id": "Fs5E6TjD3WMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_2(corpus): # был нужен для того, чтобы избавится от предложений в которых был явно использован классический пассив, отсылающий к деятелю\n",
        "    \n",
        "    check_list = []\n",
        "    for construction, type_of_construction in zip(corpus['CONSTRUCTION'], corpus['TYPE_OF_CONSTRUCTION']):\n",
        "    \n",
        "        if (construction.split('_')[0] == 'ready' and type_of_construction == 'retroactive infinitive') or (construction.split('_')[2] == 'see' and type_of_construction == 'retroactive infinitive') or (construction.split('_')[2] == 'hear' and type_of_construction == 'retroactive infinitive') or (construction.split('_')[2] == 'say' and type_of_construction == 'retroactive infinitive'):\n",
        "            check_list.append(True)\n",
        "        else:\n",
        "            check_list.append(False)\n",
        "    \n",
        "    corpus['check'] = check_list\n",
        "    corpus = corpus.drop(np.where(corpus['check'] == True)[0])\n",
        "    corpus.drop(columns = ['check'],axis = 1, inplace=True)\n",
        "    \n",
        "    return corpus"
      ],
      "metadata": {
        "id": "-rvix_oM3_Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_d = filter_2(news_d)\n",
        "scientific_d = filter_2(scientific_d)\n",
        "political_d = filter_2(political_d)\n",
        "fiction_d = filter_2(fiction_d)"
      ],
      "metadata": {
        "id": "Ok1dwWwQ4V7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_d['DISCOURSE'] = ['news']*19829 # добавление финальных меток дискурса перед склейкой в один корпус\n",
        "political_d['DISCOURSE'] = ['political']*19842\n",
        "scientific_d['DISCOURSE'] = ['scientific']*18091\n",
        "fiction_d['DISCOURSE'] = ['fiction']*13964"
      ],
      "metadata": {
        "id": "jxhAu-Jd4h5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_corpus = pd.concat([fiction_d, scientific_d, news_d, political_d], axis=0)"
      ],
      "metadata": {
        "id": "fnd4NUsc4xBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_corpus.to_csv('passive_like_construction_corpus.csv')"
      ],
      "metadata": {
        "id": "AMIiSRAU4zHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Финальный корпус доступен по ссылке: https://docs.google.com/spreadsheets/d/1dqtiHbnmVuNSn6gf0LQ97gwTIQ_HbJFi19MJQSzIS8o/edit?usp=sharing "
      ],
      "metadata": {
        "id": "zKamFrNV4z5H"
      }
    }
  ]
}